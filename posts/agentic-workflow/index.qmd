---
title: "AI Patterns I'm exploring right now"
subtitle: "From Augmentation to Orchestration (and Why Context Is the Real Battleground)"
author: "Scott Bell"
date: "2025-12-30"
draft: true
categories: 
  - LLMs
  - Azure OpenAI
  - ChatGPT
  - OpenAI
  - Gemini
  - Large Language Models
  - AI
  - GENAI
  - ai-augmented-coding
  - context
  - SLMs
  - small-language-models
---

::: {.callout-tip appearance="simple"}
This is my own original work, but I’ve had an AI model review it for clarity, tone, and grammatical errors. See my post on [AI Content Labels](../AI-Content-Labels/index.qmd)
:::

## This is a follow-up (and a reality check)

About a two weeks I wrote about how AI tools that have shaped the way I worked this last year: not in a sci-fi, *everything-is-automated* sense, but in a very grounded, *this-is-changing-how-I-think-and-build-things* way.

This post is a follow-up to that thinking.

Not a prediction piece.\
Not a hype cycle recap.

More a set of field notes on what I’m seeing *right now* and what feels may harden into some form of pattern over the next year.

If I had to summarise the theme up front:

> We’re moving away from “even bigger models doing everything”\
> and towards **better orchestration, tighter context, and smaller, more intentional systems**.

You'll think AI wrote this but it's 100% my own brains work.

> Building entropy from systems of agentic chaos

Everything below ladders up to that.

## Trend 1: AI-Augmented Coding (not “vibe coding”)

Let’s start with the most visible one.

We’re seeing a clear split between what I’d loosely call:

-   **Vibe coding / vibe engineering**
-   **AI-augmented coding**

They look similar from the outside. They are not the same thing.

### What I mean by AI-augmented coding

AI-augmented coding is about:

-   [Custom agent modes](https://code.visualstudio.com/docs/copilot/customization/custom-agents) rather than generic chat
-   Reusable prompts that encode intent, constraints, and standards
-   Treating context as a first-class engineering concern
-   Using AI to *amplify* good engineering habits, not replace them

It’s opinionated.\
It’s structured.\
And it’s usually very boring to demo which is a good sign.

### Why context compaction suddenly matters

As people build more serious workflows, they’re hitting the same wall:

-   You can’t just keep shoving more tokens into the context window
-   Bigger context ≠ better answers
-   Cost, latency, and drift all show up fast

So we’re seeing real experimentation around:

-   Context summarisation
-   Selective retrieval
-   Token compaction strategies
-   Prompt reuse with explicit boundaries

This is less about *clever prompts* and more about **engineering discipline applied to language models**.

## Trend 2: Memory as a system design problem

Once you accept that context matters, the next problem is obvious:

> How do you keep *useful* context across sessions without bloating the model?

This is where things get interesting.

### The memory problem, restated

What people want:

-   Context that persists across windows and sessions
-   Only the *relevant* parts of that context
-   No uncontrolled growth of prompt size
-   No reliance on “just remember everything”

What they don’t want:

-   Infinite chat histories
-   Giant markdown files shoved wholesale into prompts
-   Manual copy-pasting between tools

### Emerging patterns

A few patterns are starting to stabilise:

-   **Small, composable context fragments**
-   Explicit ownership of what gets remembered and why
-   Retrieval instead of retention
-   Treating memory as data, not text

Some teams are experimenting with markdown-based memory stores.\
That works — up to a point.

But the more scalable pattern is starting to look like…

## Trend 3: The rise of agentic databases

We’re now seeing the emergence of what I’d broadly call **agentic databases**.

These aren’t just vector stores bolted onto an LLM.

They’re designed specifically to support agent workflows by offering:

-   Efficient retrieval and indexing
-   Structured memory objects
-   Metadata-driven context selection
-   The ability to *collapse and expand* context dynamically

The important shift here is conceptual:

> Context doesn’t live *inside* the prompt anymore.\
> It lives *around* the agent, and is pulled in deliberately.

I’ll link to several examples of these tools below — there are already quite a few, and they’re converging on similar ideas even if the implementations differ.

## Trend 4: Sub-agents and the end of “one model does everything”

This is the one people get excited about — and also the one that’s easiest to over-engineer. I've spoken about this in the previous post.

### What sub-agents actually are (in practice)

In reality, most useful sub-agents are:

-   Small
-   Narrow
-   Slightly boring
-   Very opinionated

They exist to do *one kind of work* well.

Examples:

-   A retrieval agent
-   A validation agent
-   A data quality agent
-   A summarisation agent
-   A transformation agent

They don’t need to be autonomous or smart in a general sense.\
They need to be **predictable** and **repeatable**

### Why smaller models suddenly make sense

This is where cost and token efficiency may start to matter:

-   Smaller models are cheaper to run
-   Faster to respond
-   Less likely to over-engineer a solution
-   Often “good enough” for bounded tasks

Yes, sometimes they’re sloppy.\
But there are ways of engineering around that.

---

## Trend 5: Small Language Models aren’t a step backwards

There’s a quiet but very serious line of exploration happening around **small language models (SLMs)**.

Not everyone believes this is the future.\
That’s fine.

But the experimentation is real.

### Why people are paying attention

Small models offer:

-   Local or private deployment options
-   Much lower latency
-   Predictable cost profiles
-   Easier fine-tuning

And crucially **control**.

### Constraining output to improve performance

One of the most interesting ideas here isn’t model size at all.

It’s *constrained output*.

If you:

-   Limit the output tokens
-   Restrict the vocabulary
-   Narrow the domain aggressively

You can often:

-   Reduce hallucination
-   Improve consistency
-   Make behaviour easier to reason.
-   Make the feedback loop from the LLM's output to testing tools tighter

A former colleague James Randall has been experimenting with constrained generation (GBNF). See his fascinating write-up here https://www.jamesdrandall.com/posts/gbnf-constrained-generation/

### A concrete example: a Kimball-trained model

One example we’re actively exploring:

-   A small language model fine-tuned purely on Kimball modelling literature
-   Constrained outputs aligned to dimensional modelling concepts and terms
-   Behaves like a “Kimball-trained expert”, not a generalist

Is it flexible? No.\
Is it useful? Potentially very.

Some people believe:

> The smallest *well-constrained* fine-tuned model can outperform a much larger one in the right domain.

That idea is still being tested — but it’s not fringe anymore.

## Trend 6: Agentic swarms, voting, and orchestration

The last trend is where a lot of these ideas converge.

Instead of:

-   One agent
-   One chain of thought
-   One linear flow

We’re seeing exploration of:

-   Agentic swarms
-   Multi-voting strategies
-   Primary agents orchestrating specialised sub-agents

### From “chain of thought” to “chains of atoms”

A useful mental model being explored is moving from:

-   A single chain of thought\
    to
-   Many small, atomic reasoning paths

Each path:

-   Solves a narrow sub-problem
-   Produces a partial result
-   Feeds back into a shared context

![Agentic orchestration diagram presented at NIPS 2025](images/Aot-diagram.png)

[From this interesting paper](https://arxiv.org/abs/2502.12018)

### Agentic Swarms

Now I've been using these over the last 2/3 months after some brief astonishment at [Claude Flow.](https://github.com/ruvnet/claude-flow) Of which I have [mixed feelings about](https://github.com/ruvnet/claude-flow/issues/670#issuecomment-3612062570) This idea borrows heavily from:

-   Distributed systems
-   Swarm intelligence
-   Ensemble methods
-   Voting-based decision making

All of which existed long before LLMs.

## So what does this all point to?

If I zoom out, the pattern looks like this:

-   Less obsession with raw model size
-   More focus on orchestration
-   [Context treated as data, not text](https://github.com/humanlayer/12-factor-agents/blob/main/content/factor-03-own-your-context-window.md)
-   Memory designed, not assumed
-   Smaller, specialised agents doing bounded work

Or put another way:

> The next year of AI tooling isn’t about smarter models.\
> It’s about **better systems around them**.

And that frankly feels much more like engineering.