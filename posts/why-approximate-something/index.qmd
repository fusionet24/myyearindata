---
title: "Why approximate something with AI that you can clearly define"
author: "Scott Bell"
description: ""
date: "2025-06-20"
draft: false
categories:
  - AI
  - Mantras
  - Commonplaces
  - Patterns
---

::: {.callout-note appearance="simple"}
This was generated with the help of AI based on my research. See my post on [AI Content Labels](../AI-Content-Labels/index.qmd)
:::

Have you ever been in a meeting where a tricky business problem comes up, and someone inevitably says, "Let's just throw some AI at it"? The rush to apply AI can obscure a fundamental engineering principle: **if you can clearly define the solution to a problem, why have an algorithm try to approximate it?**

In machine learning terms, we're searching for g(x)—a probabilistic approximation of some unknown target function f(x). But what if f(x) isn't unknown? What if the logic is sitting in regulatory documents, procedure manuals, or the minds of domain experts? When you can codify clear rules and achieve near-perfect accuracy, a deterministic approach beats probabilistic AI every time.

## The Core Concept: f(x) vs g(x)

Machine learning is function approximation. Supervised learning assumes there's an ideal target function f(x) that perfectly maps inputs to outputs. A spam filter's f(x) would perfectly classify every email. A fraud detection system's f(x) would catch every fraudulent transaction.

The problem is f(x) is usually unknown. So we train models to find g(x)—the best approximation we can—using training data and performance metrics. The algorithm searches through a hypothesis space trying to get g(x) as close to f(x) as possible.

But for many business problems, f(x) isn't unknown at all. Consider:

- **Regulatory compliance**: Tax rules, healthcare regulations, financial reporting standards—these are explicitly documented deterministic processes
- **Business logic**: Pricing rules, discount calculations, approval workflows—often perfectly knowable if stakeholders align
- **RPA scenarios**: Data validation, form processing, routine calculations—prime candidates for rules-based automation

Why train an algorithm to guess at rules you already know?

::: {.callout-tip appearance="simple" collapse="true"}
## For the ML Nerds: The Math Behind f(x) vs g(x)

For a well-posed learning problem, [Tom Mitchell's classic definition](https://www.cs.cmu.edu/~tom/files/MachineLearningTomMitchell.pdf) applies: "A computer program learns from experience E with respect to task T and performance measure P, if its performance at T, measured by P, improves with experience E."

For a spam filter:
- **Task (T)**: Classifying emails as spam or not spam
- **Experience (E)**: Database of labeled emails
- **Performance (P)**: Classification accuracy percentage

The learning algorithm searches for g(x) within a hypothesis space H—the set of all possible functions the algorithm can consider. Your choice of algorithm defines this space. Linear regression gives you straight lines; decision trees give you tree structures; SVMs with polynomial kernels give you curved boundaries.

Because hypothesis spaces are often infinitely large, algorithms use inductive bias—assumptions about how to generalize beyond training data—to constrain the search. This entire apparatus rests on f(x) being unknown.

For more on learning algorithms vs hypothesis spaces, see this [Stack Exchange discussion](https://ai.stackexchange.com/questions/16746/what-is-the-difference-between-a-learning-algorithm-and-a-hypothesis). For the bias-variance tradeoff, see [Wikipedia](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff).
:::

## When Rules Beat Models

Rule-based systems explicitly codify f(x) using "if-then" logic. When the target function is knowable, they offer decisive advantages:

**Clarity**: Transparent logic you can audit and explain. No black box, no hallucinations, no [stochastic parrots](https://en.wikipedia.org/wiki/Stochastic_parrot). Critical for regulated industries.

**Certainty**: Deterministic outputs. The same input always produces the same result. No confidence scores, no probability distributions.

**Control**: Update a rule, deploy immediately. No retraining, no drift monitoring, no dataset versioning.

Rule-based automation isn't old-fashioned—it's precise. AI can help you convert domain knowledge into such systems, but it shouldn't be the system itself.

> "We often default to AI when simpler solutions exist. If you can define the target function perfectly through rules-based logic, why approximate it with machine learning? When you can codify clear rules and achieve near-perfect accuracy, a deterministic approach beats probabilistic AI every time. Sometimes f(x) is better than training g(x) to approximate it."
> *—Comment to [David Morton on AI Agents](https://www.linkedin.com/pulse/right-tool-job-david-morton-ztzkc/?trackingId=5emS3%2BUaStuTjMFped%2FFig%3D%3D)*

## The Organizational Reality

Rule-based systems fail when organizations can't agree on what f(x) is—but that's not a reason to use AI, it's a signal to clarify your process. If "the rules are too complicated to write down," you don't understand your own system well enough. AI won't fix that; it will just hide the ambiguity in a model.

## Engineer for Clarity

As problem solvers, our job is to select the right tool for the task. AI is powerful, but it's not always the answer. When business logic is clear, stable, and knowable, building f(x) directly is superior engineering.

Don't approximate what you can define.
