[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I write a few other places, the DailyDatabricks Twitter and my codementor blog\nThe Aim of this blog is to capture knowledge on technical subjects while also allowing for interesting and often pointless data exploration.\nIt’s supposed to be fun, that is all!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nazure\n\n\nadf\n\n\ndata factory\n\n\napi\n\n\nSynapse\n\n\n\n\n\n\n\n\n\n\n\nOct 29, 2022\n\n\nScott Bell\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nazure\n\n\nadf\n\n\ndata factory\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nOct 24, 2022\n\n\nScott Bell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nSep 23, 2022\n\n\nScott Bell\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/handling-uk-bank-holidays-in-adf-pipelines/index.html",
    "href": "posts/handling-uk-bank-holidays-in-adf-pipelines/index.html",
    "title": "Handling UK Bank Holidays in Azure Data Factory & Synapse",
    "section": "",
    "text": "Sometimes you have processes that you don’t need to run in certain scenarios. The past year I’ve been working with market trading data. Trading in exchanges happens every weekday but they don’t happen on weekends or national holidays as they’re usually closed.\nThis posses a problem to usual ADF trigger patterns where you can schedule quite complex triggerscenarios such as the 1st Sunday of every month. However, no concept of national holidays exist, so we need to handle this after the trigger point."
  },
  {
    "objectID": "posts/handling-uk-bank-holidays-in-adf-pipelines/index.html#problem",
    "href": "posts/handling-uk-bank-holidays-in-adf-pipelines/index.html#problem",
    "title": "Handling UK Bank Holidays in Azure Data Factory & Synapse",
    "section": "Problem",
    "text": "Problem\nSo lets restate the problem within the context of ADF. We need to trigger our process everyday, then we must validate whether this date is a UK Bank Holiday and run the business logic."
  },
  {
    "objectID": "posts/handling-uk-bank-holidays-in-adf-pipelines/index.html#solution",
    "href": "posts/handling-uk-bank-holidays-in-adf-pipelines/index.html#solution",
    "title": "Handling UK Bank Holidays in Azure Data Factory & Synapse",
    "section": "Solution",
    "text": "Solution\n\nGovernment API\nThe UK government have started to build a useful catalog of APIs for consumption, one of which contains all public holidays. The api returns the following json data.\ncurl 'https://www.gov.uk/bank-holidays.json'\nThe schema can be broken down into having two key pieces of information:\n\nThe applicable country with the values england-and-wales, scotland & northern-ireland at the root level.\nAn array of Events that contains several properties such as Title (the Holiday’s Name) and Date\n\n\n\nADF Pipeline\nSo our goal is to select the appropriate region for our data and then to filter the dates to validate our input date doesn’t match any of the ones listed in the above API.\nOur final pipeline will look like this consisting of 3 activities and parameterised so it can be reused as a child pipeline in wider orchestration flows. The code for this is re-produced at the bottom.\n\nFirst we need to setup our pipeline with the parameters and variables.\nWe need to configure a parameter to accept a date input which could be from a parent process or trigger. We have called this date\nNext we need a variable called BankHoliday to store the outcome of the question Is It a Bank Holiday for this date?\n\n\nA web activity which calls the https://www.gov.uk/bank-holidays.jsonendpoint. It is configured like so \nThe output of this should look like\nThe Filter Activity allows use to iterate through that events array and find matching items.\n\nWe must first define our items. I’ve manually selected the england-and-wales events but this could easily be made to be dynamic and extended to other regions on demand. The ADF expression for Items is\n{.adf expression .adf}  @activity('Get Bank Holidays').output['england-and-wales']['events']\nThe conditions we will use to filter these selected items is\n{.adf expression .adf} @contains(item().date, formatDateTime(pipeline().parameters.date,'yyyy-MM-dd'))\nWe check that the item contains the parameter date. We reformat the input date to ensure there are no inconsistencies in the comparison. e.g. yyyy-mm-DD != mm-yyyy-dd\nWhich outputs the following structure:\n\nItemsCount - The Number of items we searched to try and find a match\nFilteredItemsCount - The Number of Matches we found for said date\nValue - An Array of Matches for Said Date\n\nAn example of our code when we don’t find a match using the date of 2021-07-07.\n\nAn example of a successful match using the date of 2021-04-05 which is Easter Monday in the UK!\n\nYou will notice that FilteredItemsCount has incremented which is the property we will use in the next activity.\nSo now we have found our matches or not, now we need to set our variable to answer the question! We will use a set variable activity. Our logic should be pretty simple, does the FilteredItemCount = 1\n\nThe ADF expression looks like\n{.adf expression .adf} @equals(activity('Search Bank Holidays').output.FilteredItemsCount,1)\nConclusion\nNow you have a Azure Data Factory pipeline that can calculate whether a given day is a United Kingdom Bank Holiday Or Regional Bank Holiday (Wales, Scotland or Northern Ireland)."
  },
  {
    "objectID": "posts/handling-uk-bank-holidays-in-adf-pipelines/index.html#code",
    "href": "posts/handling-uk-bank-holidays-in-adf-pipelines/index.html#code",
    "title": "Handling UK Bank Holidays in Azure Data Factory & Synapse",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "posts/spcolumns-metadata-activities-in-data-factory-copy-activities-and-calculated-columns/index.html",
    "href": "posts/spcolumns-metadata-activities-in-data-factory-copy-activities-and-calculated-columns/index.html",
    "title": "Using Dynamic column metadata from sp_columns in ADF (to ignore calculated columns) while using copy activities",
    "section": "",
    "text": "Recently, I encountered an issue where we had to copy data from one Azure SQL database to another using Azure Data Factory (ADF) V2. The entire process was dynamic using parameters to select a table to be copied to the target database. A simple enough problem, right?\nWell no, firstly the table definition existed in the target DB, which given that it was a direct copy of the data shouldn’t pose a problem. However, the table definition contained calculated columns to capture the date of ingestion."
  },
  {
    "objectID": "posts/spcolumns-metadata-activities-in-data-factory-copy-activities-and-calculated-columns/index.html#problem",
    "href": "posts/spcolumns-metadata-activities-in-data-factory-copy-activities-and-calculated-columns/index.html#problem",
    "title": "Using Dynamic column metadata from sp_columns in ADF (to ignore calculated columns) while using copy activities",
    "section": "Problem",
    "text": "Problem\n\nSo now the problem becomes, “How do I exclude columns from an ADF Copy Activity based upon their metadata properties?”\n\nThe first thing that comes to mind is GET Metadata activity in ADF. Sadly that doesn’t have sufficient detail about the column definitions.\nThe next thought was the stored procedure sp_columns which returns metadata about columns and is a tool I’d imagine almost all data engineers use in the their SQL development workflow. So that’s what I did, using the (relatively) new script activity in ADF."
  },
  {
    "objectID": "posts/spcolumns-metadata-activities-in-data-factory-copy-activities-and-calculated-columns/index.html#solution",
    "href": "posts/spcolumns-metadata-activities-in-data-factory-copy-activities-and-calculated-columns/index.html#solution",
    "title": "Using Dynamic column metadata from sp_columns in ADF (to ignore calculated columns) while using copy activities",
    "section": "Solution",
    "text": "Solution\nThe pipeline used to achieve this can be found at the bottom of this post for reference. (note linked services and datasets etc are omitted)\nThe pipeline has 3 main components:\n\n\n\nADF Pipeline Architecture\n\n\n\nThe Script Activity which takes the pipeline parameter and concats sp_columns with it to retrieve table metadata like @concat('sp_columns ',pipeline().parameters.param_table)\n\nThe resulting output is json that has the following structure\n{\n    \"resultSetCount\": 1,\n    \"recordsAffected\": 0,\n    \"resultSets\": [\n        {\n            \"rowCount\": <INT>,\n            \"rows\": [\n                {\n                    \"TABLE_QUALIFIER\": <DATABASE>,\n                    \"TABLE_OWNER\": <SCHEMA>,\n                    \"TABLE_NAME\": <TABLE_NAME>,\n                    \"COLUMN_NAME\": <COLUMN_NAME>,\n                    \"DATA_TYPE\": 4,\n                    \"TYPE_NAME\": \"int\",\n                    \"PRECISION\": 10,\n                    \"LENGTH\": 4,\n                    \"SCALE\": 0,\n                    \"RADIX\": 10,\n                    \"NULLABLE\": 0,\n                    \"REMARKS\": null,\n                    \"COLUMN_DEF\": null,\n                    \"SQL_DATA_TYPE\": 4,\n                    \"SQL_DATETIME_SUB\": null,\n                    \"CHAR_OCTET_LENGTH\": null,\n                    \"ORDINAL_POSITION\": 1,\n                    \"IS_NULLABLE\": \"NO\",\n                    \"SS_DATA_TYPE\": 56\n                },\n                {<continued for every column. ...>} }\n    ]\n}\nNote :the other metadata here so this approach may have merits for other column property driven processes you need in ADF.\nFor our uses we should focus on the COLUMN_DEF property which should this be a computed column then would contain an expression and not be null!\n\nFor each entry in resultsSets[0].rows we need to iterate through them and find every column where no column_def exists which will allow us to only include the relevant columns. For this purpose we have created a pipeline variable called selected_columns of type array. We pass that into the items property to iterate @activity('Get Column Metadata').output.resultSets[0].rows\n\nInside the for each we have an If statement that checks the above condition @equals(item().COLUMN_DEF, null) and returns true when column def is null. Which then uses the append variable activity to add the column to our selected_columns array\n\n\nFinally, we that in our copy activity to build a dynamic select query by converting the array to a , separated string with the join() function. Our ADF expression looks like @concat('select ', join(variables('selected_columns'),','), ' from ', pipeline().parameters.param_table)"
  },
  {
    "objectID": "posts/spcolumns-metadata-activities-in-data-factory-copy-activities-and-calculated-columns/index.html#code",
    "href": "posts/spcolumns-metadata-activities-in-data-factory-copy-activities-and-calculated-columns/index.html#code",
    "title": "Using Dynamic column metadata from sp_columns in ADF (to ignore calculated columns) while using copy activities",
    "section": "Code",
    "text": "Code"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Back to the Data",
    "section": "",
    "text": "A lot has happened in the last few years but first I want to just get on with writing code and analysing data.\nIf you’re not familar yet, I run a (sometimes daily) Databricks Tips, tricks and hacks twitter account. That is followed by quite a few databricks people. I would recommend following it or signing up the even less frequent newsletter :)"
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Azure & Power Platform\n\n\nDatabricks\n\n\nR\n\n\nMisc"
  }
]